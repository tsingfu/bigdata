<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>

    <property require-input="true">
        <name>download_url</name>
        <value>http://yum.example.com/hadoop/apache-kylin-2.1.0-bin-hbase1x.tar.gz</value>
        <description>下载路径(只支持.tar.gz)</description>
    </property>
    <property require-input="true">
        <name>install_dir</name>
        <value>/opt/kylin</value>
        <description>安装目录</description>
    </property>
    <property require-input="true">
        <name>kylin.server.cluster-servers</name>
        <value>localhost:7070</value>
        <description>List of web servers in use, this enables one web server instance to sync up with other servers.</description>
    </property>

    <property>
        <name>kylin_pid_dir</name>
        <value>/var/run/kylin</value>
        <description>Dir containing process ID file</description>
        <value-attributes>
            <type>directory</type>
            <overridable>false</overridable>
            <editable-only-at-install>true</editable-only-at-install>
        </value-attributes>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>kylin_user</name>
        <value>kylin</value>
        <property-type>USER</property-type>
        <description>User kylin daemon runs as</description>
        <value-attributes>
            <type>user</type>
            <overridable>false</overridable>
        </value-attributes>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>kylin_group</name>
        <value>kylin</value>
        <property-type>GROUP</property-type>
        <description>kylin group</description>
        <value-attributes>
            <type>user</type>
            <overridable>false</overridable>
        </value-attributes>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>kylin_log_dir</name>
        <value>/var/log/kylin</value>
        <description>kylin Log dir</description>
        <on-ambari-upgrade add="true"/>
    </property>

    <property>
        <name>kylin_env_content</name>
        <display-name>kylin-env template</display-name>
        <description>This is the jinja template for kylin-env file</description>
        <value>
export HADOOP_HOME=/opt/hadoop
export HADOOP_CONF_DIR=/etc/hadoop
export HBASE_CONF_DIR=/etc/hbase
export HIVE_CONF=/etc/hive
export HIVE_HOME=/opt/hive
export HCAT_HOME=/opt/hive
export KYLIN_HOME=/opt/kylin
export KYLIN_CONF_DIR=/etc/kylin
export ZOOKEEPER_HOME=/opt/zookeeper
export SPARK_HOME=/opt/spark
export SPARK_CONF_DIR=/etc/spark2
        </value>
        <value-attributes>
            <type>content</type>
        </value-attributes>
        <on-ambari-upgrade add="true"/>
    </property>

    <property>
        <name>kylin-server-log4j</name>
        <display-name>kylin server log4j template</display-name>
        <description>kylin server log4j </description>
        <value>
#define appenders
log4j.appender.file=org.apache.log4j.DailyRollingFileAppender
log4j.appender.file.layout=org.apache.log4j.PatternLayout
log4j.appender.file.File={{kylin_log_dir}}/kylin.log
log4j.appender.file.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}:%L : %m%n
log4j.appender.file.Append=true

#overall config
log4j.rootLogger=INFO,file
log4j.logger.org.apache.kylin=DEBUG
log4j.logger.org.springframework=WARN
log4j.logger.org.springframework.security=INFO
        </value>
        <value-attributes>
            <type>content</type>
        </value-attributes>
        <on-ambari-upgrade add="true"/>
    </property>

    <property>
        <name>kylin-tools-log4j</name>
        <display-name>kylin tools log4j template</display-name>
        <description>kylin tools log4j file</description>
        <value>
# the kylin-tools-log4j.properties is mainly for configuring log properties on kylin tools, including:
#   1. tools launched by kylin.sh script, e.g. DeployCoprocessorCLI
#   2. DebugTomcat
#   3. others
#
# It's called kylin-tools-log4j.properties so that it won't distract users from the other more important log4j config file: kylin-server-log4j.properties
# enable this by -Dlog4j.configuration=kylin-tools-log4j.properties

log4j.rootLogger=INFO,stderr

log4j.appender.stderr=org.apache.log4j.ConsoleAppender
log4j.appender.stderr.Target=System.err
log4j.appender.stderr.layout=org.apache.log4j.PatternLayout
log4j.appender.stderr.layout.ConversionPattern=%d{ISO8601} %-5p [%t %c{1}:%L]: %m%n

#log4j.logger.org.apache.hadoop=ERROR
log4j.logger.org.apache.kylin=DEBUG
log4j.logger.org.springframework=WARN
log4j.logger.org.apache.kylin.tool.shaded=INFO
        </value>
        <value-attributes>
            <type>content</type>
        </value-attributes>
        <on-ambari-upgrade add="true"/>
    </property>

    <property>
        <name>kylin-check-env-content</name>
        <display-name>kylincheck-env template</display-name>
        <description>kylin check-env file</description>
        <value><![CDATA[
source $(cd -P -- "$(dirname -- "$0")" && pwd -P)/header.sh
source $(cd -P -- "$(dirname -- "$0")" && pwd -P)/find-hadoop-conf-dir.sh

if [ -z "${kylin_hadoop_conf_dir}" ]; then
hadoop_conf_param=
else
hadoop_conf_param="--config ${kylin_hadoop_conf_dir}"
fi

if [ -z "$KYLIN_HOME" ]
then
quit 'Please make sure KYLIN_HOME has been set'
else
echo "KYLIN_HOME is set to ${KYLIN_HOME}"
fi

if [ -z "$(command -v hbase version)" ]
then
quit "Please make sure the user has the privilege to run hbase shell"
fi

if [ -z "$(command -v hive --version)" ]
then
quit "Please make sure the user has the privilege to run hive shell"
fi

if [ -z "$(command -v hadoop version)" ]
then
quit "Please make sure the user has the privilege to run hadoop shell"
fi

WORKING_DIR=`bash $KYLIN_HOME/bin/get-properties.sh kylin.env.hdfs-working-dir`
if [ -z "$WORKING_DIR" ]
then
quit "Please set kylin.env.hdfs-working-dir in kylin.properties"
fi
            ]]></value>
        <value-attributes>
            <type>content</type>
        </value-attributes>
        <on-ambari-upgrade add="true"/>
    </property>


    <property>
        <name>content</name>
        <display-name>kylin.properties template</display-name>
        <description>This is the jinja template for kylin.properties file</description>
        <value>
### METADATA | ENV ###

# The metadata store in hbase
kylin.metadata.url=kylin_metadata@hbase

# Working folder in HDFS, make sure user has the right access to the hdfs directory
kylin.env.hdfs-working-dir=/user/kylin

# DEV|QA|PROD. DEV will turn on some dev features, QA and PROD has no difference in terms of functions.
kylin.env=PROD


### SERVER | WEB ###

# Kylin server mode, valid value [all, query, job]
kylin.server.mode={{server_mode}}

# List of web servers in use, this enables one web server instance to sync up with other servers.
kylin.server.cluster-servers={{kylin_cluster_servers}}

# Display timezone on UI,format like[GMT+N or GMT-N]
kylin.web.timezone=GMT+8

kylin.web.cross-domain-enabled=true

### SOURCE ###

# Hive client, valid value [cli, beeline]
kylin.source.hive.client=cli

# Parameters for beeline client, only necessary if hive client is beeline
#kylin.source.hive.beeline-params=-n root --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' -u jdbc:hive2://localhost:10000

kylin.source.hive.keep-flat-table=false

# Hive database name for putting the intermediate flat tables
kylin.source.hive.database-for-flat-table=default

# Whether redistribute the intermediate flat table before building
kylin.source.hive.redistribute-flat-table=true


### STORAGE ###

# The storage for cube is hbase
kylin.storage.url=hbase

# Compression codec for htable, valid value [none, snappy, lzo, gzip, lz4]
kylin.storage.hbase.compression-codec=lzo

# HBase Cluster FileSystem, which serving hbase, format as hdfs://hbase-cluster:8020
# Leave empty if hbase running on same cluster with hive and mapreduce
#kylin.storage.hbase.cluster-fs=

# The cut size for hbase region, in GB.
kylin.storage.hbase.region-cut-gb=5

# The hfile size of GB, smaller hfile leading to the converting hfile MR has more reducers and be faster.
# Set 0 to disable this optimization.
kylin.storage.hbase.hfile-size-gb=2

kylin.storage.hbase.min-region-count=1
kylin.storage.hbase.max-region-count=500

# Optional information for the owner of kylin platform, it can be your team's email
# Currently it will be attached to each kylin's htable attribute
kylin.storage.hbase.owner-tag=bigdata@example.com

kylin.storage.hbase.coprocessor-mem-gb=3

# By default kylin can spill query's intermediate results to disks when it's consuming too much memory.
# Set it to false if you want query to abort immediately in such condition.
kylin.storage.partition.aggr-spill-enabled=true

# The maximum number of bytes each coprocessor is allowed to scan.
# To allow arbitrary large scan, you can set it to 0.
kylin.storage.partition.max-scan-bytes=3221225472

# The default coprocessor timeout is (hbase.rpc.timeout * 0.9) / 1000 seconds,
# You can set it to a smaller value. 0 means use default.
# kylin.storage.hbase.coprocessor-timeout-seconds=0


### JOB ###

# Max job retry on error, default 0: no retry
kylin.job.retry=0

# Max count of concurrent jobs running
kylin.job.max-concurrent-jobs=10

# The percentage of the sampling, default 100%
kylin.job.sampling-percentage=100

# Whether get job status from resource manager with kerberos authentication
kylin.job.status.with.kerberos=true

# Timeout in seconds
kylin.job.step.timeout=7200

# If true, will send email notification on job complete
#kylin.job.notification-enabled=true
#kylin.job.notification-mail-enable-starttls=true
#kylin.job.notification-mail-host=smtp.office365.com
#kylin.job.notification-mail-port=587
#kylin.job.notification-mail-username=kylin@example.com
#kylin.job.notification-mail-password=mypassword
#kylin.job.notification-mail-sender=kylin@example.com


### ENGINE ###

# Time interval to check hadoop job status
kylin.engine.mr.yarn-check-interval-seconds=10

kylin.engine.mr.reduce-input-mb=500

kylin.engine.mr.max-reducer-number=500

kylin.engine.mr.mapper-input-rows=1000000

# Enable dictionary building in MR reducer
kylin.engine.mr.build-dict-in-reducer=true

# Number of reducers for fetching UHC column distinct values
kylin.engine.mr.uhc-reducer-count=1

### CUBE | DICTIONARY ###

# 'auto', 'inmem' or 'layer'
kylin.cube.algorithm=auto

# A smaller threshold prefers layer, a larger threshold prefers in-mem
kylin.cube.algorithm.layer-or-inmem-threshold=7

kylin.cube.aggrgroup.max-combination=4096

kylin.snapshot.max-mb=300


### QUERY ###

# Controls the maximum number of bytes a query is allowed to scan storage.
# The default value 0 means no limit.
# The counterpart kylin.storage.partition.max-scan-bytes sets the maximum per coprocessor.
kylin.query.max-scan-bytes=0

kylin.query.udf.version=org.apache.kylin.query.udf.VersionUDF
kylin.query.udf.concat=org.apache.kylin.query.udf.ConcatUDF

kylin.query.cache-enabled=true


### SECURITY ###

# Spring security profile, options: testing, ldap, saml
# with "testing" profile, user can use pre-defined name/pwd like KYLIN/ADMIN to login
kylin.security.profile=testing


### Spark Engine Configs ###

# Hadoop conf folder, will export this as "HADOOP_CONF_DIR" to run spark-submit
# This must contain site xmls of core, yarn, hive, and hbase in one folder
kylin.env.hadoop-conf-dir=/etc/hadoop

# Estimate the RDD partition numbers
kylin.engine.spark.rdd-partition-cut-mb=10

# Minimal partition numbers of rdd
kylin.engine.spark.min-partition=1

# Max partition numbers of rdd
kylin.engine.spark.max-partition=5000

## Spark conf (default is in spark/conf/spark-defaults.conf)
kylin.engine.spark-conf.spark.master=yarn
kylin.engine.spark-conf.spark.submit.deployMode=cluster
kylin.engine.spark-conf.spark.yarn.queue=default
kylin.engine.spark-conf.spark.executor.memory=1G
kylin.engine.spark-conf.spark.executor.cores=2
kylin.engine.spark-conf.spark.executor.instances=1
kylin.engine.spark-conf.spark.eventLog.enabled=true
kylin.engine.spark-conf.spark.eventLog.dir=hdfs\:///user/kylin/spark-history
kylin.engine.spark-conf.spark.history.fs.logDirectory=hdfs\:///user/kylin/spark-history
        </value>
        <value-attributes>
            <type>content</type>
        </value-attributes>
        <on-ambari-upgrade add="true"/>
    </property>

    <property>
        <name>kylin.server.kerberos.principal</name>
        <value/>
        <value-attributes>
            <empty-value-valid>true</empty-value-valid>
        </value-attributes>
        <description>
            Kerberos principal name for the kylin.
        </description>
        <property-type>KERBEROS_PRINCIPAL</property-type>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>kylin.server.kerberos.keytab</name>
        <value/>
        <value-attributes>
            <empty-value-valid>true</empty-value-valid>
        </value-attributes>
        <description>
            Location of the kerberos keytab file for the kylin.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
</configuration>
